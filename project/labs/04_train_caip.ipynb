{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ML model on Cloud AI Platform\n",
    "\n",
    "This notebook shows how to:\n",
    "* Export training code from [a Keras notebook](../06_feateng_keras/solution/taxifare_fc.ipynb) into a trainer file\n",
    "* Create a Docker container based on a [DLVM container](https://cloud.google.com/ai-platform/deep-learning-containers/docs/kubernetes-container])\n",
    "* Deploy training job to cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Export the data from BigQuery to GCS\n",
    "1. Navigate to [export_data.ipynb](export_data.ipynb)\n",
    "2. Update 'your-gcs-project-here' to your GCP project name\n",
    "3. Run all the notebook cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Edit notebook parameters\n",
    "1. Navigate to [notebook_params.yaml](notebook_params.yaml)\n",
    "2. Replace the bucket name with your own bucket containing your model (likely gcp-project with -ml at the end)\n",
    "3. Save the notebook\n",
    "4. Return to this notebook and continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export code from notebook\n",
    "\n",
    "This notebook extracts code from a notebook and creates a Python file suitable for use as model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import nbformat\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "def write_parameters(cell_source, params_yaml, outfp):\n",
    "    with open(params_yaml, 'r') as ifp:\n",
    "        y = yaml.safe_load(ifp)\n",
    "        # print out all the lines in notebook\n",
    "        write_code(cell_source, 'PARAMS from notebook', outfp)\n",
    "        # print out YAML file; this will override definitions above\n",
    "        formats = [\n",
    "            '{} = {}', # for integers and floats\n",
    "            '{} = \"{}\"', # for strings\n",
    "        ]\n",
    "        write_code(\n",
    "            '\\n'.join([\n",
    "                formats[type(value) is str].format(key, value) for key, value in y.items()]),\n",
    "            'PARAMS from YAML',\n",
    "            outfp\n",
    "        )\n",
    "\n",
    "def write_code(cell_source, comment, outfp):\n",
    "    lines = cell_source.split('\\n')\n",
    "    if len(lines) > 0 and lines[0].startswith('%%'):\n",
    "        prefix = '#'\n",
    "    else:\n",
    "        prefix = ''\n",
    "    \n",
    "    print(\"### BEGIN {} ###\".format(comment), file=outfp)\n",
    "    for line in lines:\n",
    "        line = prefix + line.replace('print(', 'logging.info(')\n",
    "        if len(line) > 0 and (line[0] == '!' or line[0] == '%'):\n",
    "            print('#' + line, file=outfp)\n",
    "        else:\n",
    "            print(line, file=outfp)\n",
    "    print(\"### END {} ###\\n\".format(comment), file=outfp)\n",
    "            \n",
    "def convert_notebook(notebook_filename, params_yaml, outfp):\n",
    "    write_code('import logging', 'code added by notebook conversion', outfp)\n",
    "    with open(INPUT) as ifp:\n",
    "        nb = nbformat.reads(ifp.read(), nbformat.NO_CONVERT)\n",
    "        for cell in nb.cells:\n",
    "            if cell.cell_type == 'code':\n",
    "                if 'tags' in cell.metadata and 'display' in cell.metadata.tags:\n",
    "                    logging.info('Ignoring cell # {} with display tag'.format(cell.execution_count))\n",
    "                elif 'tags' in cell.metadata and 'parameters' in cell.metadata.tags:\n",
    "                    logging.info('Writing params cell # {}'.format(cell.execution_count))\n",
    "                    write_parameters(cell.source, PARAMS, outfp)\n",
    "                else:\n",
    "                    logging.info('Writing model cell # {}'.format(cell.execution_count))\n",
    "                    write_code(cell.source, 'Cell #{}'.format(cell.execution_count), outfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "INPUT='../../06_feateng_keras/solution/taxifare_fc.ipynb'\n",
    "PARAMS='./notebook_params.yaml'\n",
    "OUTDIR='./container/trainer'\n",
    "\n",
    "!mkdir -p $OUTDIR\n",
    "OUTFILE=os.path.join(OUTDIR, 'model.py')\n",
    "!touch $OUTDIR/__init__.py\n",
    "with open(OUTFILE, 'w') as ofp:\n",
    "    #convert_notebook(INPUT, PARAMS, sys.stdout)\n",
    "    convert_notebook(INPUT, PARAMS, ofp)\n",
    "#!cat $OUTFILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out model file\n",
    "\n",
    "<b>Note</b> Once the training starts, __Interrupt the Kernel__ (from the notebook ribbon bar above). Because it processes the entire dataset, this will take a long time on the relatively small machine on which you are running Notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 $OUTFILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Docker container\n",
    "\n",
    "Package up the trainer file into a Docker container and submit the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile container/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-cpu\n",
    "\n",
    "#RUN python3 -m pip install --upgrade --quiet tf-nightly-2.0-preview\n",
    "RUN python3 -m pip install --upgrade --quiet cloudml-hypertune\n",
    "\n",
    "COPY trainer /trainer\n",
    "CMD [\"python3\", \"/trainer/model.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile container/push_docker.sh\n",
    "export PROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\n",
    "export IMAGE_REPO_NAME=serverlessml_training_container\n",
    "#export IMAGE_TAG=$(date +%Y%m%d_%H%M%S)\n",
    "#export IMAGE_URI=gcr.io/$PROJECT_ID/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "export IMAGE_URI=gcr.io/$PROJECT_ID/$IMAGE_REPO_NAME\n",
    "\n",
    "echo \"Building  $IMAGE_URI\"\n",
    "docker build -f Dockerfile -t $IMAGE_URI ./\n",
    "echo \"Pushing $IMAGE_URI\"\n",
    "docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note</b>: If you get a permissions error when running push_docker.sh from Notebooks, do it from CloudShell:\n",
    "* Open [CloudShell](https://console.cloud.google.com/cloudshell) on the GCP Console\n",
    "*  ```git clone https://github.com/GoogleCloudPlatform/training-data-analyst```\n",
    "*  ```cd training-data-analyst/quests/serverlessml/07_caip/solution/container```\n",
    "*  ```bash push_docker.sh```\n",
    "\n",
    "This next step takes 5 - 10 minutes to run\n",
    "\n",
    "##### TODO: What is going on here? Why are you doing ML in Docker? Does everyone in your group undrstand what are the advantages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd container\n",
    "bash push_docker.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to AI Platform\n",
    "\n",
    "Submit a training job using this custom container that we have just built. After you submit the job, [monitor it here](https://console.cloud.google.com/ai-platform/jobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "JOBID=serverlessml_$(date +%Y%m%d_%H%M%S)\n",
    "REGION=us-central1\n",
    "PROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\n",
    "BUCKET=$(gcloud config list project --format \"value(core.project)\")-ml\n",
    "\n",
    "#IMAGE=gcr.io/deeplearning-platform-release/tf2-cpu\n",
    "IMAGE=gcr.io/$PROJECT_ID/serverlessml_training_container\n",
    "\n",
    "gcloud beta ai-platform jobs submit training $JOBID \\\n",
    "   --staging-bucket=gs://$BUCKET  --region=$REGION \\\n",
    "   --master-image-uri=$IMAGE \\\n",
    "   --master-machine-type=n1-standard-4 --scale-tier=CUSTOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training job will take 35 - 45 minutes to complete on the dataset. You can cancel the job once you confirm it started and have inspected the logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
